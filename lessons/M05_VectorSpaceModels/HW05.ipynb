{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376b42c2-27de-4f8f-832d-573204c67a35",
   "metadata": {},
   "source": [
    "# HW 05: BOW and TFIDF\n",
    "\n",
    "Charlie Perez (cwp5xyj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f68777-835d-45f7-b600-243ad175efe8",
   "metadata": {},
   "source": [
    "### Part 1: Everything I need to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8065e773-cc86-4f49-9ca3-d69931d992d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126b6502-f593-473d-aa8c-c955798b6e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_home = config['DEFAULT']['data_home'] \n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "data_prefix = 'austen-melville'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfbb950-a8aa-44b5-b934-0a57b5252604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num']\n",
    "\n",
    "LIB = pd.read_csv(f\"{output_dir}/{data_prefix}-LIB.csv\").set_index('book_id')\n",
    "TOKEN = pd.read_csv(f'{output_dir}/{data_prefix}-CORPUS.csv').set_index(OHCO).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467effc5-546b-4a3e-98fc-4d40717ddd67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id\n",
       "105       83613\n",
       "121       77586\n",
       "141      160366\n",
       "158      160884\n",
       "161      119858\n",
       "946       23115\n",
       "1212      33241\n",
       "1342     122089\n",
       "1900     108015\n",
       "2701     215461\n",
       "4045     102347\n",
       "8118     119230\n",
       "10712    143251\n",
       "13720     96874\n",
       "13721    102078\n",
       "15422     65510\n",
       "15859     75232\n",
       "21816     95169\n",
       "34970    155024\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN.reset_index().book_id.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8d8025-8166-4c9c-a372-3141f35b0021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I already created this vocab table for the Austen-Melville set, so just gonna read it in\n",
    "VOCAB = pd.read_csv(f'{output_dir}/{data_prefix}-VOCAB.csv').set_index('term_str').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54775f15-f5f8-4fcb-955a-20c7c8df0a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>n_pos_group</th>\n",
       "      <th>cat_pos_group</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>cat_pos</th>\n",
       "      <th>stop</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>stem_lancaster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9.713651e-07</td>\n",
       "      <td>19.973483</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CD'}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CD'}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.117070e-05</td>\n",
       "      <td>16.449921</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>2</td>\n",
       "      <td>{'CD', 'NN'}</td>\n",
       "      <td>3</td>\n",
       "      <td>{'CD', 'NNP', 'NN'}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.914095e-06</td>\n",
       "      <td>18.388520</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CD'}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CD'}</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9.713651e-07</td>\n",
       "      <td>19.973483</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CD'}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CD'}</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9.713651e-07</td>\n",
       "      <td>19.973483</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CD'}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'CD'}</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n  n_chars             p          i max_pos max_pos_group  \\\n",
       "term_str                                                               \n",
       "0          2        1  9.713651e-07  19.973483      CD            CD   \n",
       "1         23        1  1.117070e-05  16.449921      CD            CD   \n",
       "10         6        2  2.914095e-06  18.388520      CD            CD   \n",
       "100        2        3  9.713651e-07  19.973483      CD            CD   \n",
       "1000       2        4  9.713651e-07  19.973483      CD            CD   \n",
       "\n",
       "          n_pos_group cat_pos_group  n_pos              cat_pos  stop  \\\n",
       "term_str                                                                \n",
       "0                   1        {'CD'}      1               {'CD'}     0   \n",
       "1                   2  {'CD', 'NN'}      3  {'CD', 'NNP', 'NN'}     0   \n",
       "10                  1        {'CD'}      1               {'CD'}     0   \n",
       "100                 1        {'CD'}      1               {'CD'}     0   \n",
       "1000                1        {'CD'}      1               {'CD'}     0   \n",
       "\n",
       "         stem_porter stem_snowball stem_lancaster  \n",
       "term_str                                           \n",
       "0                  0             0              0  \n",
       "1                  1             1              1  \n",
       "10                10            10             10  \n",
       "100              100           100            100  \n",
       "1000            1000          1000           1000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efcb47d2-126e-4312-b0a8-a304309a5c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">105</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>('Sir', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Sir</td>\n",
       "      <td>sir</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('Walter', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Walter</td>\n",
       "      <td>walter</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('Elliot,', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Elliot,</td>\n",
       "      <td>elliot</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('of', 'IN')</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('Kellynch', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Kellynch</td>\n",
       "      <td>kellynch</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       pos_tuple  pos  \\\n",
       "book_id chap_id para_num sent_num token_num                             \n",
       "105     1       1        0        0               ('Sir', 'NNP')  NNP   \n",
       "                                  1            ('Walter', 'NNP')  NNP   \n",
       "                                  2           ('Elliot,', 'NNP')  NNP   \n",
       "                                  3                 ('of', 'IN')   IN   \n",
       "                                  4          ('Kellynch', 'NNP')  NNP   \n",
       "\n",
       "                                            token_str  term_str pos_group  \n",
       "book_id chap_id para_num sent_num token_num                                \n",
       "105     1       1        0        0               Sir       sir        NN  \n",
       "                                  1            Walter    walter        NN  \n",
       "                                  2           Elliot,    elliot        NN  \n",
       "                                  3                of        of        IN  \n",
       "                                  4          Kellynch  kellynch        NN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c172aa1-6820-417c-af17-e5979679670d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "      <th>book_len</th>\n",
       "      <th>n_chaps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>/home/cwp5xyj/Documents/MSDS/DS5001/data/auste...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>PERSUASION</td>\n",
       "      <td>^Chapter\\s+\\d+$</td>\n",
       "      <td>83624</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>/home/cwp5xyj/Documents/MSDS/DS5001/data/auste...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>NORTHANGER ABBEY</td>\n",
       "      <td>^CHAPTER\\s+\\d+$</td>\n",
       "      <td>77601</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>/home/cwp5xyj/Documents/MSDS/DS5001/data/auste...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>MANSFIELD PARK</td>\n",
       "      <td>^CHAPTER\\s+[IVXLCM]+$</td>\n",
       "      <td>160378</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>/home/cwp5xyj/Documents/MSDS/DS5001/data/auste...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>EMMA</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\s*$</td>\n",
       "      <td>160926</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>/home/cwp5xyj/Documents/MSDS/DS5001/data/auste...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>SENSE AND SENSIBILITY</td>\n",
       "      <td>^CHAPTER\\s+\\d+$</td>\n",
       "      <td>119873</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file_path        author  \\\n",
       "book_id                                                                    \n",
       "105      /home/cwp5xyj/Documents/MSDS/DS5001/data/auste...  AUSTEN, JANE   \n",
       "121      /home/cwp5xyj/Documents/MSDS/DS5001/data/auste...  AUSTEN, JANE   \n",
       "141      /home/cwp5xyj/Documents/MSDS/DS5001/data/auste...  AUSTEN, JANE   \n",
       "158      /home/cwp5xyj/Documents/MSDS/DS5001/data/auste...  AUSTEN, JANE   \n",
       "161      /home/cwp5xyj/Documents/MSDS/DS5001/data/auste...  AUSTEN, JANE   \n",
       "\n",
       "                         title                   chap_regex  book_len  n_chaps  \n",
       "book_id                                                                         \n",
       "105                 PERSUASION              ^Chapter\\s+\\d+$     83624       24  \n",
       "121           NORTHANGER ABBEY              ^CHAPTER\\s+\\d+$     77601       31  \n",
       "141             MANSFIELD PARK        ^CHAPTER\\s+[IVXLCM]+$    160378       48  \n",
       "158                       EMMA  ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$    160926       55  \n",
       "161      SENSE AND SENSIBILITY              ^CHAPTER\\s+\\d+$    119873       50  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ee331-a100-4e8e-b905-f2e2bf9d0e79",
   "metadata": {},
   "source": [
    "### Part 2: The Questions\n",
    "\n",
    "#### Question 1: BOW and TFIDF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f79780-954f-4092-b1ee-28da00b2d8ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# things look fine\n",
    "# let's write a BOW function\n",
    "\n",
    "def bag_of_words(tokens, bag):\n",
    "    \n",
    "    # we're just gonna instantiate stuff within the function just in case\n",
    "    OHCO = ['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num']\n",
    "    bags = dict(\n",
    "        SENTS = OHCO[:4],\n",
    "        PARAS = OHCO[:3],\n",
    "        CHAPS = OHCO[:2],\n",
    "        BOOKS = OHCO[:1]\n",
    "    )\n",
    "    \n",
    "    # bit of error handling (mostly just for practice)\n",
    "    bag = bag.upper()\n",
    "    if bag not in bags:\n",
    "        raise ValueError(F'Invalid input: Value must be one of {list(bags.keys())}')\n",
    "        \n",
    "        \n",
    "    BOW = tokens.groupby(bags[bag]+['term_str']).term_str.count().to_frame('n')\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a269e20-e1d4-4870-b782-7cda8d9c29cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write the TFIDF function\n",
    "# this is going to contain a lot more\n",
    "\n",
    "def TFIDF_from_BOW(BOW, tf_method):\n",
    "    # 'parameters' that we aren't gonna make variable for this function\n",
    "    tf_norm_k = .5\n",
    "    gradient_cmap = 'YlGnBu'\n",
    "    \n",
    "    # first make DTCM and calculate N\n",
    "    DTCM = BOW.n.unstack(fill_value=0)\n",
    "    N = DTCM.shape[0]\n",
    "    \n",
    "    tf_methods = {\n",
    "        'sum': (DTCM.T / DTCM.T.sum()).T,\n",
    "        'max': (DTCM.T / DTCM.T.max()).T,\n",
    "        'log': (np.log2(1 + DTCM.T)).T,\n",
    "        'raw':  DTCM,\n",
    "        'double_norm': (DTCM.T / DTCM.T.max()).T,\n",
    "        'binary': DTCM.T.astype('bool').astype('int').T\n",
    "    }\n",
    "    \n",
    "    # just for fun\n",
    "    tf_method = tf_method.lower()\n",
    "    if tf_method not in tf_methods:\n",
    "        raise ValueError(F'Invalid input: Value must be one of {list(tf_methods.keys())}')\n",
    "    \n",
    "    TF = tf_methods[tf_method]\n",
    "    DF = DTCM.astype('bool').sum()\n",
    "    IDF = np.log2(N / DF) # not variable according to assignment rules, but could easily be\n",
    "    TFIDF = TF * IDF\n",
    "    \n",
    "    return TFIDF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e13fac09-a6d6-4b6e-8556-e6ca843f6260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I can also print the functions by typing something like the below\n",
    "# so if this was what you wanted, here's proof I know how to do it\n",
    "# but no need to show things twice ya know? just takes up space, so I commented it out\n",
    "\n",
    "\n",
    "# bag_of_words??\n",
    "# TFIDF_from_BOW??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425595a1-58c4-48fd-9ada-9ac23f2f4d6f",
   "metadata": {},
   "source": [
    "#### Question 2: Top 20 words using 'max', 'book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d76110b-3a34-498a-b75d-da67bb31d664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "term_str\n",
       "elinor        0.642969\n",
       "vernon        0.493614\n",
       "darcy         0.366742\n",
       "reginald      0.351225\n",
       "frederica     0.341733\n",
       "crawford      0.337235\n",
       "elliot        0.324016\n",
       "weston        0.315232\n",
       "pierre        0.293657\n",
       "knightley     0.288490\n",
       "tilney        0.262482\n",
       "elton         0.259316\n",
       "bingley       0.252012\n",
       "wentworth     0.243650\n",
       "courcy        0.242061\n",
       "woodhouse     0.225281\n",
       "churchhill    0.218329\n",
       "marianne      0.202796\n",
       "babbalanja    0.173393\n",
       "mainwaring    0.170866\n",
       "Name: tfidf, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_q2 = bag_of_words(TOKEN, 'BOOKS')\n",
    "\n",
    "TFIDF_q2 = TFIDF_from_BOW(BOW_q2, 'max')\n",
    "\n",
    "BOW_q2['tfidf'] = TFIDF_q2.stack()\n",
    "BOW_q2 = BOW_q2.groupby('term_str')['tfidf'].mean().sort_values(ascending=False).head(20)\n",
    "BOW_q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea06049-31a4-4745-b335-87fe25c8669a",
   "metadata": {},
   "source": [
    "All names. Makes sense I think."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f82899-a3ca-409e-82be-1ec31d9e2acd",
   "metadata": {},
   "source": [
    "#### Question 3: Top 20 using 'sum', 'chapter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8296821-9946-40a2-b9e5-6ec70c57b666",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "term_str\n",
       "hypothetical     0.962653\n",
       "slushing         0.769841\n",
       "charmers         0.584106\n",
       "tusculan         0.576389\n",
       "disputations     0.576389\n",
       "um               0.560363\n",
       "inquest          0.515494\n",
       "unbends          0.467914\n",
       "increases        0.413067\n",
       "communion        0.395583\n",
       "confuting        0.330378\n",
       "consents         0.293096\n",
       "moredock         0.290542\n",
       "moot             0.259887\n",
       "transact         0.228927\n",
       "introduces       0.228702\n",
       "metamorphosis    0.212492\n",
       "forgiver         0.209160\n",
       "plujii           0.205909\n",
       "ugh              0.202758\n",
       "Name: tfidf, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_q3 = bag_of_words(TOKEN, 'CHAPS')\n",
    "\n",
    "TFIDF_q3 = TFIDF_from_BOW(BOW_q3, 'sum')\n",
    "\n",
    "BOW_q3['tfidf'] = TFIDF_q3.stack()\n",
    "BOW_q3 = BOW_q3.groupby('term_str')['tfidf'].mean().sort_values(ascending=False).head(20)\n",
    "BOW_q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817921e1-72b6-4967-87af-f01f90cac82f",
   "metadata": {},
   "source": [
    "#### Question 4: Difference in Q2 and Q3 answer by POS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f4b2a67-dba4-4839-8e33-041096a31045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_pos\n",
      "NNP    20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "max_pos\n",
      "NNP    7\n",
      "NN     6\n",
      "NNS    2\n",
      "JJ     2\n",
      "VBZ    2\n",
      "VB     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# save the q2 and q3 responses in memory\n",
    "q2_ans = pd.DataFrame(BOW_q2)\n",
    "q3_ans = pd.DataFrame(BOW_q3)\n",
    "\n",
    "q2_ans = q2_ans.join(VOCAB['max_pos'], on = 'term_str', how = 'left')\n",
    "q3_ans = q3_ans.join(VOCAB['max_pos'], on = 'term_str', how = 'left')\n",
    "\n",
    "print(q2_ans['max_pos'].value_counts())\n",
    "print('\\n')\n",
    "print(q3_ans['max_pos'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34a946b4-9092-47be-8400-c4740f423810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>max_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hypothetical</th>\n",
       "      <td>0.962653</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slushing</th>\n",
       "      <td>0.769841</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charmers</th>\n",
       "      <td>0.584106</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tusculan</th>\n",
       "      <td>0.576389</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disputations</th>\n",
       "      <td>0.576389</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>um</th>\n",
       "      <td>0.560363</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inquest</th>\n",
       "      <td>0.515494</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unbends</th>\n",
       "      <td>0.467914</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increases</th>\n",
       "      <td>0.413067</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communion</th>\n",
       "      <td>0.395583</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confuting</th>\n",
       "      <td>0.330378</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consents</th>\n",
       "      <td>0.293096</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moredock</th>\n",
       "      <td>0.290542</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moot</th>\n",
       "      <td>0.259887</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transact</th>\n",
       "      <td>0.228927</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>introduces</th>\n",
       "      <td>0.228702</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamorphosis</th>\n",
       "      <td>0.212492</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forgiver</th>\n",
       "      <td>0.209160</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plujii</th>\n",
       "      <td>0.205909</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ugh</th>\n",
       "      <td>0.202758</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tfidf max_pos\n",
       "term_str                       \n",
       "hypothetical   0.962653     NNP\n",
       "slushing       0.769841     NNP\n",
       "charmers       0.584106     NNS\n",
       "tusculan       0.576389     NNP\n",
       "disputations   0.576389      NN\n",
       "um             0.560363      JJ\n",
       "inquest        0.515494     NNP\n",
       "unbends        0.467914      NN\n",
       "increases      0.413067     VBZ\n",
       "communion      0.395583      NN\n",
       "confuting      0.330378     NNP\n",
       "consents       0.293096     NNS\n",
       "moredock       0.290542     NNP\n",
       "moot           0.259887      NN\n",
       "transact       0.228927      VB\n",
       "introduces     0.228702     VBZ\n",
       "metamorphosis  0.212492      NN\n",
       "forgiver       0.209160      NN\n",
       "plujii         0.205909     NNP\n",
       "ugh            0.202758      JJ"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddf61c-24ec-4949-a800-5149c5bf6327",
   "metadata": {},
   "source": [
    "Referencing the UPenn tagset, the results of Q2 are all proper nouns, while the results of Q3 include common nouns, adjectives, and verbs. I think the supposed proper nouns from Q3 may be miscategorized, though ('hypothetical' and 'slushing' are not often proper nouns, but 'plujii' appears in both lists). But I am also using max_pos and not the specific word's POS. This seemed necessary as the words from Q2 are used many times, and it seemed excessive to regenerate POS tags for the filtered data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0476ca-3b02-47ed-a3e5-100b79df0722",
   "metadata": {},
   "source": [
    "#### Question 5: Who has the most significant adjective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4a3c1e7-9fc5-4e3d-b56c-172157bd3563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start by splitting TOKEN table by author\n",
    "\n",
    "TOKEN_q5 = TOKEN.join(LIB['author'], on = 'book_id', how = 'left')\n",
    "TOKEN_AUSTEN = TOKEN_q5[TOKEN_q5['author']=='AUSTEN, JANE']\n",
    "TOKEN_MELVILLE = TOKEN_q5[TOKEN_q5['author']=='MELVILLE, HERMAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "effdd27a-46dd-4dee-a613-9161263aa0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then, run everything for each author\n",
    "\n",
    "BOW_AUSTEN = bag_of_words(TOKEN_AUSTEN, 'CHAPS')\n",
    "TFIDF_AUSTEN = TFIDF_from_BOW(BOW_AUSTEN, 'max')\n",
    "BOW_AUSTEN['tfidf'] = TFIDF_AUSTEN.stack()\n",
    "\n",
    "BOW_MELVILLE = bag_of_words(TOKEN_MELVILLE, 'CHAPS')\n",
    "TFIDF_MELVILLE = TFIDF_from_BOW(BOW_MELVILLE, 'max')\n",
    "BOW_MELVILLE['tfidf'] = TFIDF_MELVILLE.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89626b6b-3d7b-434c-87c6-8ce671428888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I know if there isn't a \"JJ\" in here, I'll have to change this, but it's fine\n",
    "AUSTEN = pd.DataFrame(BOW_AUSTEN.groupby('term_str')['tfidf'].mean().sort_values(ascending=False))\n",
    "MELVILLE = pd.DataFrame(BOW_MELVILLE.groupby('term_str')['tfidf'].mean().sort_values(ascending=False))\n",
    "\n",
    "AUSTEN = AUSTEN.join(VOCAB['max_pos'], on = 'term_str', how = 'left').dropna()\n",
    "MELVILLE = MELVILLE.join(VOCAB['max_pos'], on = 'term_str', how = 'left').dropna()\n",
    "\n",
    "AUSTEN = AUSTEN[AUSTEN['max_pos'].str.startswith('JJ')]\n",
    "MELVILLE = MELVILLE[MELVILLE['max_pos'].str.startswith('JJ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e50516ca-ea61-49e6-adfe-448fe50c66e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tfidf max_pos\n",
      "term_str                     \n",
      "undismayed   2.095926      JJ\n",
      "precarious   0.762155      JJ\n",
      "dreary       0.698642      JJ\n",
      "eoconomical  0.493159      JJ\n",
      "unmajestic   0.493159      JJ\n",
      "\n",
      "\n",
      "                tfidf max_pos\n",
      "term_str                     \n",
      "ugh          4.462786      JJ\n",
      "um           2.886301      JJ\n",
      "manchineels  1.497387      JJ\n",
      "sneezes      1.342485      JJ\n",
      "adorable     1.226297      JJ\n"
     ]
    }
   ],
   "source": [
    "print(AUSTEN.head())\n",
    "print('\\n')\n",
    "print(MELVILLE.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba15d5-cdcf-43c5-ba2e-6d9b18e78c68",
   "metadata": {},
   "source": [
    "According to this, Melville's work has the most significant adjective if we break it down like this, but I'm a bit unsure if I would consider \"ugh\" or \"um\" to be an adjective. Same with \"machineels\" and \"sneezes\". I think Austen has the most significant adjective with \"undismayed\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
